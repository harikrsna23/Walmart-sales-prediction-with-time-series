# -*- coding: utf-8 -*-
"""Walmart forecast Capstone Project 2 methods.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q7wLkhSfl5Z1s0VnWyELaLF8JKbZyvki

### Problem Statement 1:
A retail store that has multiple outlets across the country are facing issues in managing the
inventory - to match the demand with respect to supply.

## Start by Importing the required libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""### Import the dataset"""

df=pd.read_csv("Walmart.csv")
df

"""##### You are provided with the weekly sales data for their various outlets. Use statistical analysis, EDA, outlier analysis, and handle the missing values to come up with various insights that can give them a clear perspective on the following:

## Exploratory Data Analysis
"""

df.shape

df.info()

df.isnull().sum()

df.describe() #Summary Statistics for Numerical Variables

df['Date'] = pd.to_datetime(df['Date'])
df['Month'] = df['Date'].dt.month
df['Week'] = df['Date'].dt.week
df['Year'] = df['Date'].dt.year
df.head()

"""### Univariate Analysis

##### Numarical Columns:
Weekly_Sales, Temperature, Fuel_Price, CPI, Unemployment

##### Categorical columns:
Store, Holiday_Flag
"""

plt.hist(df['Weekly_Sales'], bins='auto', color='blue', alpha=0.7, rwidth=0.85)
plt.xlabel('Sales')
plt.ylabel('Frequency')
plt.title('Histogram of Weekly Sales')
plt.show()

sns.kdeplot(df['Weekly_Sales'])
plt.xlabel('Sales')
plt.title('KDE of Weekly Sales')
plt.show()

df['Weekly_Sales'].skew()

sns.boxplot(x=df['Weekly_Sales'])
plt.xlabel('Variable')
plt.ylabel('Sales')
plt.title('Box Plot of Weekly Sales')
plt.show()

"""Inference: Weekly Sales column have some Outliers, lets see if these outliers are due to any reason."""

Q1 = df['Weekly_Sales'].quantile(0.25)
Q3 = df['Weekly_Sales'].quantile(0.75)
# Calculate IQR
IQR = Q3 - Q1
# Identify outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers_WS = df[(df['Weekly_Sales'] < lower_bound) | (df['Weekly_Sales'] > upper_bound)]

print("Upper Bound:", upper_bound)
print("Number of Outliers:", len(outliers_WS))

#Taking 2.7e+06 as the point above upper bound for the month of Oct, Nov, Dec
outliers_WS_yearend=df[(df['Weekly_Sales'] > 2.7e+06) & ((df['Date'].dt.month == 11) | (df['Date'].dt.month == 12) | (df['Date'].dt.month == 10))].sort_values(by='Weekly_Sales', ascending=False)
outliers_WS_yearend.shape

df[df['Weekly_Sales'] > 2.7e+06].sort_values(by='Weekly_Sales',ascending=True)

"""**Conclusion:**

Outliers in weekly sales,are identified during October, November, and December, align with the expected year-end surge in consumer spending. Given the seasonal nature of these fluctuations, it is recommended not to remove these outliers from the dataset.

### Bivariate analysis
"""

plt.figure(figsize=(16,5))
sns.pointplot(x ='Store', y = "Weekly_Sales", data = df)
plt.title('Store Vs Sales')
plt.show()

month_wise_sales = pd.pivot_table(df, values = "Weekly_Sales", columns = "Year", index = "Month")
month_wise_sales.plot()

"""##### a. If the weekly sales are affected by the unemployment rate, if yes - which stores are suffering the most?"""

sns.lineplot(x="Store", y="Unemployment", data=df)
plt.show()

sns.kdeplot(df['Unemployment'])

df['Weekly_Sales'].corr(df['Unemployment'])

"""The relationship between 'Weekly_Sales' and 'Unemployment' is not very strong based on this correlation value.

**Conclusion:**
Limited impact of unemployment on weekly sales

##### b. If the weekly sales show a seasonal trend, when and what could be the reason?
"""

plt.figure(figsize=(12, 6))
sns.lineplot(x='Date', y='Weekly_Sales', data=df)
plt.title('Weekly Sales Over Time')
plt.xlabel('Date')
plt.ylabel('Weekly Sales')
plt.show()

from scipy.stats import f_oneway

result_anova = f_oneway(*[group['Weekly_Sales'] for name, group in df.groupby('Month')])
print("ANOVA p-value:", result_anova.pvalue)

"""*The small p-value suggests that there are statistically significant differences in weekly sales among different months.*

The results of the ANOVA test strengthen the inference that weekly sales exhibit a seasonal pattern.
"""

df['Month'] = df['Date'].dt.month
plt.figure(figsize=(12, 6))
sns.boxplot(x='Month', y='Weekly_Sales', data=df)
plt.title('Box Plot of Weekly Sales by Month')
plt.xlabel('Month')
plt.ylabel('Weekly Sales')
plt.show()

"""**Conclusion:**

The surge in weekly sales during the last three months is likely attributed to the holiday shopping season, festive atmosphere, and various cultural and economic factors. Consumers tend to increase spending during this period, leading to a significant impact on retail sales.

##### c. Does temperature affect the weekly sales in any manner?
"""

df['Temperature'].corr(df['Weekly_Sales'])

plt.figure(figsize=(10, 6))
sns.scatterplot(x='Temperature', y='Weekly_Sales', data=df)
plt.title('Scatter Plot: Temperature vs Weekly Sales')
plt.xlabel('Temperature')
plt.ylabel('Weekly Sales')
plt.show()

from scipy.stats import pearsonr
# Perform Pearson correlation test
corr_coeff, p_value = pearsonr(df['Temperature'], df['Weekly_Sales'])

print("Pearson Correlation Coefficient:", corr_coeff)
print("p-value:", p_value)

"""**Inference:**

There is a statistically significant weak negative correlation between temperature and weekly sales. As the temperature decreases, there is a slight tendency for weekly sales to decrease, and vice versa.

**Conclusion:**

The evidence from the correlation analysis suggests that temperature may have a modest impact on weekly sales. However, it's important to note that the correlation coefficient is small, and the relationship is weak.

##### d. How is the Consumer Price index affecting the weekly sales of various stores?
"""

correlations = []

for store_num, group in df.groupby('Store'):
    correlation = group['CPI'].corr(group['Weekly_Sales'])
    correlations.append((store_num, correlation))

correlation_df = pd.DataFrame(correlations, columns=['Store', 'Correlation'])
df = df.merge(correlation_df, on='Store', how='left')
print("Correlation between CPI and Weekly Sales for each store:")
print(correlation_df)

df.head()

positively_correlated = (correlation_df['Correlation'] > 0).sum()
negatively_correlated = (correlation_df['Correlation'] < 0).sum()

plt.figure(figsize=(10, 6))
bars=plt.bar(['Positive Correlation', 'Negative Correlation'], [positively_correlated, negatively_correlated], color=['green', 'red'])
plt.bar_label(bars, fmt='%d', label_type='edge', fontsize=10)
plt.title('Distribution of Correlations with CPI')
plt.xlabel('Correlation Types')
plt.ylabel('Number of Stores')
plt.show()

selected_stores =input().split(',')
selected_stores= [int(store_num) for store_num in selected_stores]
for store_num in selected_stores:
    sns.scatterplot(x='CPI', y='Weekly_Sales', data=df[df['Store'] == store_num])
    plt.title(f'Scatter Plot: CPI vs Weekly Sales - Store {store_num}')
    plt.xlabel('CPI')
    plt.ylabel('Weekly Sales')
    plt.show()

"""**Conclusion**:
A majority of stores (25 out of 45) exhibit a positive correlation between the Consumer Price Index (CPI) and weekly sales. This suggests that, in these stores, higher CPI values tend to correspond with increased weekly sales. On the other hand, 20 stores demonstrate a negative correlation, indicating that higher CPI values are associated with decreased weekly sales in those specific stores.

##### e. Top performing stores according to the historical data.
"""

total_sales_by_store = df.groupby('Store')['Weekly_Sales'].sum()
top_performing_stores = total_sales_by_store.nlargest(5)
print("Top Performing Stores based on Total Weekly Sales:")
print(top_performing_stores)

"""##### f. The worst performing store, and how significant is the difference between the highest and lowest performing stores"""

worst_performing_store = total_sales_by_store.idxmin()
worst_performing_sales = total_sales_by_store.min()
best_performing_store = total_sales_by_store.idxmax()
best_performing_sales = total_sales_by_store.max()
difference_highest_lowest = best_performing_sales - worst_performing_sales

print("Worst Performing Store:")
print(f"Store: {worst_performing_store}, Total Weekly Sales: {worst_performing_sales:.2f}")

print("\nBest Performing Store:")
print(f"Store: {best_performing_store}, Total Weekly Sales: {best_performing_sales:.2f}")

print("\nDifference Between Highest and Lowest Performing Stores:")
print(f"{difference_highest_lowest:.2f}")

"""### Preprocessing"""



df.set_index('Date', inplace=True)
# There are about 45 different stores in this dataset. Lets select the any store id from 1-45
a= int(input("Enter the store id:"))
store = df[df.Store == a]
sales = pd.DataFrame(store.Weekly_Sales.groupby(store.index).sum())
sales.dtypes

sales.head(20)

sales.Weekly_Sales.plot(figsize=(10,6), title= 'Weekly Sales of a Store', fontsize=14, color = 'blue')
plt.show()

from statsmodels.tsa.seasonal import seasonal_decompose

decomposition = seasonal_decompose(sales.Weekly_Sales, period=12)
fig = plt.figure()
fig = decomposition.plot()
fig.set_size_inches(12, 10)
plt.show()

### Testing For Stationarity

from statsmodels.tsa.stattools import adfuller

test_result=adfuller(sales['Weekly_Sales'])

#Ho: It is non stationary
#H1: It is stationary

def adfuller_test(sales):
    result=adfuller(sales)
    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']
    for value,label in zip(result,labels):
        print(label+' : '+str(value) )
    if result[1] <= 0.05:
        print("strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary")
    else:
        print("weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary ")

adfuller_test(sales['Weekly_Sales'])

"""### Model 1

"""

# Define the p, d and q parameters to take any value between 0 and 2
p = d = q = range(0, 5)
import itertools
# Generate all different combinations of p, d and q triplets
pdq = list(itertools.product(p, d, q))

# Generate all different combinations of seasonal p, d and q triplets
seasonal_pdq = [(x[0], x[1], x[2], 52) for x in list(itertools.product(p, d, q))]

print(pdq)

"""train the data with the best model using aic or  bic

"""

import statsmodels.api as sm

mod = sm.tsa.statespace.SARIMAX(sales.Weekly_Sales,
                                order=(4, 4, 3),
                                seasonal_order=(1, 1, 0, 52),   #enforce_stationarity=False,
                                enforce_invertibility=False)

results = mod.fit()

print(results.summary().tables[1])

plt.style.use('seaborn-pastel')
results.plot_diagnostics(figsize=(15, 12))
plt.show()

pred = results.get_prediction(start=pd.to_datetime('2010-01-10'), dynamic=False)
pred_ci = pred.conf_int()

sales

ax = sales.Weekly_Sales['2010':].plot(label='observed',color='c')
pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7,color='orange')

ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='k', alpha=.2)

ax.set_xlabel('Time Period')
ax.set_ylabel('Sales')
plt.legend()

plt.show()

y_forecasted = pred.predicted_mean
y_truth = sales.Weekly_Sales['2012-12-10':]

# Compute the mean square error
mse = ((y_forecasted - y_truth) ** 2).mean()
print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))

pred_dynamic = results.get_prediction(start=pd.to_datetime('2012-12-10'), dynamic=True, full_results=True)
pred_dynamic_ci = pred_dynamic.conf_int()

ax = sales.Weekly_Sales['2010':].plot(label='observed', figsize=(12, 8))
pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)

ax.fill_between(pred_dynamic_ci.index,
                pred_dynamic_ci.iloc[:, 0],
                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)

ax.fill_betweenx(ax.get_ylim(), pd.to_datetime('2012-12-10'), sales.Weekly_Sales.index[-1],
                 alpha=.1, zorder=-1)

ax.set_xlabel('Time Period')
ax.set_ylabel('Sales')

plt.legend()
plt.show()

import numpy as np
# Extract the predicted and true values of our time series
y_forecasted = pred_dynamic.predicted_mean
print(y_forecasted)

y1=sales.Weekly_Sales

y_truth = y1['2012-12-10':]

print(y_truth)

# Compute the Root mean square error
rmse = np.sqrt(((y_forecasted - y_truth) ** 2).mean())
print('The Root Mean Squared Error of our forecasts is {}'.format(round(rmse, 2)))

Residual= y_forecasted - y_truth
print("Residual for Store1",np.abs(Residual).sum())

# Get forecast 12 weeks ahead in future
pred_uc = results.get_forecast(steps=12)

print(pred_uc)

# Get confidence intervals of forecasts
pred_ci = pred_uc.conf_int()

ax = y1.plot(label='observed', figsize=(12, 8))
pred_uc.predicted_mean.plot(ax=ax, label='Forecast')
ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='k', alpha=.25)
ax.set_xlabel('Time Period')
ax.set_ylabel('Sales')

plt.legend()
plt.show()





"""### Method 2"""

from pandas.plotting import autocorrelation_plot
autocorrelation_plot(sales['Weekly_Sales'])
plt.show()

from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
import statsmodels.api as sm

fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
fig = sm.graphics.tsa.plot_acf(sales['Weekly_Sales'],lags=40,ax=ax1)
ax2 = fig.add_subplot(212)
fig = sm.graphics.tsa.plot_pacf(sales['Weekly_Sales'],lags=40,ax=ax2)

import statsmodels.api as sm

mod = sm.tsa.statespace.SARIMAX(sales,
                                order=(3, 1, 3),
                                seasonal_order=(3, 1, 3, 52),
                                enforce_stationarity=False,
                                enforce_invertibility=False)
results=mod.fit()



sales['forecast'] = results.predict(start='2012-01-27', end='2012-12-10', dynamic=True)
sales[['Weekly_Sales', 'forecast']].plot(figsize=(12, 8))

sales = sales.drop('forecast', axis=1)

sales.shape

sales.tail(40)







